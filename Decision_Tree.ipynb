{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Questions >\n"
      ],
      "metadata": {
        "id": "hgCOALjyid_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.1 What is a Decision Tree, and how does it work?"
      ],
      "metadata": {
        "id": "ne6FOaGLitPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.1 A decision tree is a supervised machine learning algorithm used for both classification and regression tasks.  It works by recursively partitioning the data based on features, creating a tree-like structure where each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (for classification) or a predicted value (for regression).\n",
        "\n",
        "Here's a breakdown of how it works:\n",
        "\n",
        "1. **Root Node:** The process starts at the root node, which represents the entire dataset.\n",
        "\n",
        "2. **Feature Selection:** The algorithm selects the best feature to split the data based on a criterion (e.g., Gini impurity, information gain, or entropy).  The \"best\" feature provides the most information to separate the data into distinct classes or predict the target variable effectively.\n",
        "\n",
        "3. **Splitting:** The dataset is split into subsets based on the values of the chosen feature. Each subset corresponds to a branch emanating from the current node.\n",
        "\n",
        "4. **Recursion:** Steps 2 and 3 are recursively applied to each subset (or child node) until a stopping criterion is met.  Stopping criteria can include:\n",
        "\n",
        "    * **Maximum depth:** Limiting the depth of the tree to prevent overfitting.\n",
        "    * **Minimum samples per leaf:** Ensuring that each leaf node has a minimum number of samples to avoid creating nodes with very small and potentially noisy data.\n",
        "    * **Information gain threshold:**  Setting a threshold for the minimum information gain required to split a node further.\n",
        "    * **Purity of leaf node:**  The node contains only one class or reaches a certain purity level.\n",
        "\n",
        "\n",
        "5. **Leaf Nodes:**  The terminal nodes of the tree are called leaf nodes. Each leaf node assigns a class label (classification) or a predicted value (regression) to the instances that reach that node.\n",
        "\n",
        "6. **Prediction:** To make a prediction for a new instance, the algorithm traverses the tree from the root node, following the branches corresponding to the instance's feature values until it reaches a leaf node. The class label or predicted value associated with that leaf node is then assigned to the instance.\n",
        "\n",
        "\n",
        "**Example:** Imagine predicting whether someone will play tennis based on weather conditions. The decision tree might first split the data based on \"Outlook\" (Sunny, Overcast, Rainy). Then, based on the \"Outlook\" value, it might further split the data based on \"Humidity\" or \"Wind.\"  The leaf nodes will represent a decision like \"Yes\" (play tennis) or \"No\" (don't play tennis).\n",
        "\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "* **Overfitting:** Decision trees can be prone to overfitting, meaning they learn the training data too well and don't generalize well to unseen data. Techniques like pruning, limiting tree depth, or using ensemble methods (Random Forests, Gradient Boosting) can mitigate this.\n",
        "* **Interpretability:**  One advantage of decision trees is their interpretability. The tree structure clearly shows the decision-making process.\n",
        "\n",
        "\n",
        "In summary, decision trees are powerful and versatile algorithms that provide a clear and intuitive way to model complex relationships within data, making them valuable tools for various prediction and classification tasks.\n"
      ],
      "metadata": {
        "id": "lUt9UWY0m1i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.2 What are impurity measures in Decision Trees?"
      ],
      "metadata": {
        "id": "WTh5KXy3nG7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.2 Impurity measures in decision trees quantify the homogeneity of a set of samples.  A lower impurity score indicates that the samples in a node are mostly of the same class (for classification) or have similar target values (for regression), while a higher impurity score suggests a mix of different classes or values.  Decision trees aim to minimize impurity at each split.\n",
        "\n",
        "Common impurity measures include:\n",
        "\n",
        "* **Gini Impurity:** Measures the probability of misclassifying a randomly chosen element in a set if it were randomly labeled according to the class distribution in the set.  A Gini impurity of 0 indicates perfect purity (all samples belong to the same class), while a Gini impurity of 1 means maximum impurity (all classes are equally represented).\n",
        "\n",
        "* **Entropy:** Measures the uncertainty or randomness in a set of samples.  It's based on information theory and quantifies the average amount of information needed to identify the class label of a randomly selected sample.  Like Gini impurity, an entropy of 0 represents perfect purity, and higher values indicate greater impurity.\n",
        "\n",
        "* **Classification Error:**  Calculates the proportion of misclassified samples in a set.  While simple, it's less sensitive to changes in class probabilities compared to Gini impurity or entropy, which makes it less effective for finding the optimal splits in decision trees.\n",
        "\n",
        "The choice of impurity measure can influence the resulting decision tree, but often the differences are minor. Gini impurity and entropy are generally preferred due to their sensitivity to class probabilities.\n"
      ],
      "metadata": {
        "id": "OwTV114Tnaow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.3 What is the mathematical formula for Gini Impurity?"
      ],
      "metadata": {
        "id": "fRceZrCgnmID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.3 The mathematical formula for Gini Impurity is:\n",
        "\n",
        "Gini impurity = 1 - Σ (pᵢ)²\n",
        "\n",
        "where:\n",
        "\n",
        "*   **pᵢ** represents the probability of an element belonging to class *i* in the dataset or subset.\n",
        "*   The summation (Σ) is over all possible classes *i*.\n",
        "\n",
        "\n",
        "In simpler terms, it calculates the probability of incorrectly classifying a randomly chosen element from the set if you randomly labeled it according to the class distribution in the same set.  A Gini impurity of 0 indicates perfect purity (all samples belong to the same class), while a Gini impurity of 1 means maximum impurity (all classes are equally represented).\n"
      ],
      "metadata": {
        "id": "iwiFAmnXn8lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.4 What is the mathematical formula for Entropy?"
      ],
      "metadata": {
        "id": "jUWVBmhBoFAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.4 The formula for entropy, particularly in the context of information theory, is given by:\n",
        "\n",
        "\\[\n",
        "H(X) = - \\sum_{i=1}^{n} p(x_i) \\log_b p(x_i)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\( H(X) \\) is the entropy of the random variable \\( X \\), which represents the amount of uncertainty or information.\n",
        "- \\( p(x_i) \\) is the probability of outcome \\( x_i \\).\n",
        "- \\( \\log_b \\) is the logarithm to the base \\( b \\). Typically, base 2 is used when measuring entropy in bits.\n",
        "- The sum is taken over all possible outcomes \\( x_i \\) of the random variable \\( X \\).\n",
        "\n",
        "This formula quantifies the expected amount of \"surprise\" or information content from the possible outcomes of a random variable.\n",
        "\n",
        "In the context of thermodynamics, entropy is similarly related to the number of possible microscopic configurations of a system:\n",
        "\n",
        "\\[\n",
        "S = k_B \\ln \\Omega\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\( S \\) is the entropy.\n",
        "- \\( k_B \\) is the Boltzmann constant.\n",
        "- \\( \\Omega \\) is the number of possible microscopic states (microstates) of the system.\n",
        "\n",
        "This thermodynamic entropy describes the disorder or randomness within a physical system."
      ],
      "metadata": {
        "id": "D3Jibf9Jo_ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.5 What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "RckEXWAgpIuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.5 **Information Gain** is a metric used to measure the effectiveness of a feature (or attribute) in classifying a dataset, particularly in the construction of decision trees for classification tasks. It quantifies the reduction in entropy (or uncertainty) achieved by partitioning the data based on a particular attribute. The goal is to select the attribute that best splits the data into subsets that are as pure (or homogenous) as possible, with respect to the target variable.\n",
        "\n",
        "### Formula for Information Gain:\n",
        "\n",
        "Information gain is calculated using the following formula:\n",
        "\n",
        "\\[\n",
        "\\text{Information Gain}(D, A) = H(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} H(D_v)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( D \\) is the dataset.\n",
        "- \\( A \\) is the attribute (feature) being evaluated.\n",
        "- \\( \\text{Values}(A) \\) are the possible values of the attribute \\( A \\).\n",
        "- \\( D_v \\) is the subset of data for which attribute \\( A \\) has value \\( v \\).\n",
        "- \\( |D_v| \\) is the number of elements in the subset \\( D_v \\).\n",
        "- \\( |D| \\) is the total number of elements in the dataset \\( D \\).\n",
        "- \\( H(D) \\) is the entropy of the entire dataset \\( D \\).\n",
        "- \\( H(D_v) \\) is the entropy of the subset \\( D_v \\).\n",
        "\n",
        "### Steps for Calculating Information Gain:\n",
        "\n",
        "1. **Calculate the Entropy of the Entire Dataset**:\n",
        "   First, you need to calculate the entropy of the dataset before any splitting. This gives you a measure of the disorder or uncertainty of the class labels in the dataset.\n",
        "   \n",
        "   \\[\n",
        "   H(D) = - \\sum_{i=1}^{n} p(c_i) \\log_2 p(c_i)\n",
        "   \\]\n",
        "   Where \\( p(c_i) \\) is the probability of each class \\( c_i \\) in the dataset.\n",
        "\n",
        "2. **Split the Dataset Based on the Attribute**:\n",
        "   Next, you evaluate how splitting the dataset by each possible value of the attribute \\( A \\) reduces the uncertainty. This is done by calculating the entropy for each subset of data \\( D_v \\) formed by the split.\n",
        "\n",
        "3. **Calculate the Weighted Sum of Entropies of Subsets**:\n",
        "   For each value of the attribute, calculate the entropy of the resulting subset \\( D_v \\), and weight it by the proportion of instances in that subset.\n",
        "\n",
        "4. **Compute the Information Gain**:\n",
        "   Finally, subtract the weighted sum of the entropies of the subsets from the original entropy of the dataset to get the information gain.\n",
        "\n",
        "### Example of How Information Gain is Used in Decision Trees:\n",
        "\n",
        "1. **Choosing the Best Attribute**:\n",
        "   During the construction of a decision tree, the algorithm evaluates all possible attributes and calculates the information gain for each. The attribute that results in the highest information gain is selected as the \"best\" attribute for splitting the dataset at the current node.\n",
        "\n",
        "2. **Recursive Splitting**:\n",
        "   After splitting based on the chosen attribute, the process is repeated recursively for each subset of data (created from the split), calculating the information gain for all remaining attributes. This continues until a stopping criterion is met (e.g., all data points are classified, or no further improvement can be made).\n",
        "\n",
        "### Why Use Information Gain in Decision Trees?\n",
        "\n",
        "- **Maximizes Class Purity**: By selecting the attribute with the highest information gain, decision trees aim to produce branches where each node has as pure a classification as possible, i.e., where the majority of instances belong to a single class.\n",
        "- **Efficient Classifications**: Using attributes that maximize information gain allows the tree to classify data more efficiently by reducing uncertainty at each step.\n",
        "  \n",
        "In summary, information gain is a central concept in decision trees, helping to select the attribute that best divides the dataset into pure subsets. It helps build a tree structure that can classify new instances with high accuracy."
      ],
      "metadata": {
        "id": "9LZSCIqKpkbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.6 What is the difference between Gini Impurity and Entropy?"
      ],
      "metadata": {
        "id": "KcnKNND8ppK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.6 **Gini Impurity** and **Entropy** are both criteria used to evaluate the \"impurity\" or \"uncertainty\" of a dataset when constructing decision trees. They are used to measure how well a feature splits the data into different classes. Both aim to find the best feature to split the data into the purest (or least uncertain) groups. While they serve the same purpose, they differ in the way they calculate and interpret the impurity.\n",
        "\n",
        "### 1. **Gini Impurity**\n",
        "\n",
        "Gini Impurity is a measure of how often a randomly chosen element from the dataset would be incorrectly classified if it were randomly labeled according to the distribution of labels in the dataset.\n",
        "\n",
        "#### Formula for Gini Impurity:\n",
        "\n",
        "\\[\n",
        "Gini(D) = 1 - \\sum_{i=1}^{n} p(c_i)^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( p(c_i) \\) is the probability of class \\( c_i \\) in the dataset.\n",
        "- \\( n \\) is the number of classes.\n",
        "\n",
        "- **Range of Gini Impurity**: Gini impurity ranges from 0 to 1. A Gini value of 0 indicates that all elements in the dataset belong to a single class (perfectly pure), while a Gini value of 1 indicates the maximum possible impurity (completely mixed classes).\n",
        "  \n",
        "#### Key Points about Gini Impurity:\n",
        "- It is faster to compute than entropy because it doesn't require logarithmic calculations.\n",
        "- It tends to prefer larger, more balanced splits over smaller, purer splits (though this is a subtle difference).\n",
        "- The Gini impurity is often used in algorithms like **CART (Classification and Regression Trees)**.\n",
        "\n",
        "### 2. **Entropy**\n",
        "\n",
        "Entropy is a concept borrowed from information theory and measures the amount of uncertainty or disorder in a dataset. It quantifies the unpredictability of the class labels in the dataset.\n",
        "\n",
        "#### Formula for Entropy:\n",
        "\n",
        "\\[\n",
        "H(D) = - \\sum_{i=1}^{n} p(c_i) \\log_2 p(c_i)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( p(c_i) \\) is the probability of class \\( c_i \\) in the dataset.\n",
        "- \\( n \\) is the number of classes.\n",
        "\n",
        "- **Range of Entropy**: Entropy also ranges from 0 to \\( \\log_2 n \\) (where \\( n \\) is the number of possible classes). A value of 0 means the dataset is perfectly pure (i.e., all instances belong to a single class), while a higher entropy indicates more uncertainty or disorder.\n",
        "\n",
        "#### Key Points about Entropy:\n",
        "- Entropy is more computationally expensive than Gini impurity because it involves calculating logarithms.\n",
        "- It tends to be more sensitive to changes in the class distribution.\n",
        "- It is often used in algorithms like **ID3** and **C4.5**.\n",
        "\n",
        "### **Key Differences Between Gini Impurity and Entropy**\n",
        "\n",
        "1. **Mathematical Formula**:\n",
        "   - **Gini Impurity**: \\( Gini(D) = 1 - \\sum_{i=1}^{n} p(c_i)^2 \\)\n",
        "   - **Entropy**: \\( H(D) = - \\sum_{i=1}^{n} p(c_i) \\log_2 p(c_i) \\)\n",
        "   \n",
        "2. **Range**:\n",
        "   - **Gini Impurity**: Ranges from 0 (pure) to 1 (impure).\n",
        "   - **Entropy**: Ranges from 0 (pure) to \\( \\log_2 n \\) (most impure), where \\( n \\) is the number of classes.\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - **Gini Impurity**: Measures the probability that a randomly selected element will be incorrectly classified.\n",
        "   - **Entropy**: Measures the level of uncertainty or unpredictability of the dataset.\n",
        "\n",
        "4. **Preference in Splitting**:\n",
        "   - **Gini Impurity** tends to select the best split based on the impurity of the group, while **Entropy** considers the amount of information (or surprise) produced by a split.\n",
        "\n",
        "5. **Computation Speed**:\n",
        "   - **Gini Impurity**: Generally faster because it doesn't require logarithms.\n",
        "   - **Entropy**: More computationally expensive due to the need for logarithmic calculations.\n",
        "\n",
        "6. **Sensitivity to Distribution**:\n",
        "   - **Gini Impurity** is less sensitive to changes in the class distribution when compared to Entropy.\n",
        "   - **Entropy** is more sensitive and often prefers splits that maximize information gain (i.e., produce the most significant reduction in uncertainty).\n",
        "\n",
        "### **When to Use Each Criterion:**\n",
        "- **Gini Impurity** is generally preferred in practice because it is faster and simpler to compute, and it often produces similar decision trees to entropy-based measures.\n",
        "- **Entropy** can be used if you want a more theoretically grounded approach, particularly when the model needs to account for information theory.\n",
        "\n",
        "### **Practical Example:**\n",
        "\n",
        "Consider a dataset with two classes, \\( A \\) and \\( B \\), with the following distribution:\n",
        "\n",
        "- \\( 70\\% \\) of the data points belong to class \\( A \\)\n",
        "- \\( 30\\% \\) of the data points belong to class \\( B \\)\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        "\\[\n",
        "Gini(D) = 1 - (0.7^2 + 0.3^2) = 1 - (0.49 + 0.09) = 1 - 0.58 = 0.42\n",
        "\\]\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "\\[\n",
        "H(D) = -[0.7 \\log_2 0.7 + 0.3 \\log_2 0.3] = -[0.7(-0.51457) + 0.3(-1.73697)] = 0.7(0.51457) + 0.3(1.73697) = 0.36019 + 0.52109 = 0.88128\n",
        "\\]\n",
        "\n",
        "So in this example, Gini impurity (0.42) is lower than the entropy (0.88), indicating that Gini impurity is a simpler, less complex metric, though the choice between the two may not significantly affect the performance of a decision tree.\n",
        "\n",
        "### Conclusion:\n",
        "- **Gini Impurity** and **Entropy** both serve the same purpose in decision tree construction, but Gini is simpler and computationally faster, while entropy provides a more detailed, information-theoretic perspective. Both criteria are widely used, and the choice between them often comes down to the specific requirements of the application or algorithm preference."
      ],
      "metadata": {
        "id": "7s78yUKVp8Sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.7 What is the mathematical explanation behind Decision Trees?"
      ],
      "metadata": {
        "id": "NtfM0tAcqB-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.7 A **Decision Tree** is a supervised machine learning algorithm that can be used for both classification and regression tasks. The goal of a decision tree is to recursively split the data into subsets based on the features, resulting in a tree-like structure where each internal node represents a decision based on a feature, each branch represents the outcome of that decision, and each leaf node represents the final prediction or class label.\n",
        "\n",
        "### Mathematical Explanation of Decision Trees\n",
        "\n",
        "1. **Recursive Partitioning**:\n",
        "   The core idea of a decision tree is to split the dataset into subsets (or regions) based on feature values. The goal is to recursively partition the data in a way that the instances in each subset are as homogeneous as possible with respect to the target variable (class label for classification or numerical value for regression).\n",
        "\n",
        "2. **Tree Structure**:\n",
        "   A decision tree can be represented as a binary tree (although multi-way trees are possible), where:\n",
        "   - **Root Node**: Represents the entire dataset.\n",
        "   - **Internal Nodes**: Each node represents a decision point based on a feature. For example, if the feature \\(X_i\\) is greater than a threshold \\(t\\), the data is split into two subsets.\n",
        "   - **Leaf Nodes**: The final decision or prediction is made here (class label for classification or predicted value for regression).\n",
        "\n",
        "### Key Concepts in the Construction of Decision Trees\n",
        "\n",
        "#### 1. **Splitting Criteria**:\n",
        "   To build a decision tree, we must select the feature to split on at each internal node. The choice of the splitting feature is based on a criterion that measures how \"pure\" or \"homogeneous\" the resulting subsets are. The most common criteria are **Gini Impurity**, **Entropy**, and **Variance Reduction** (for regression).\n",
        "\n",
        "#### 2. **Impurity Measures**:\n",
        "   These measures determine the quality of a split. A good split is one where the data is divided into subsets that are as pure as possible, meaning that the instances in each subset belong to the same class or have similar values. Some common impurity measures include:\n",
        "\n",
        "   - **Gini Impurity** (for classification):\n",
        "     \n",
        "     \\[\n",
        "     Gini(D) = 1 - \\sum_{i=1}^{n} p(c_i)^2\n",
        "     \\]\n",
        "     Where \\( p(c_i) \\) is the proportion of class \\( c_i \\) in dataset \\( D \\).\n",
        "\n",
        "   - **Entropy** (for classification):\n",
        "     \n",
        "     \\[\n",
        "     H(D) = - \\sum_{i=1}^{n} p(c_i) \\log_2 p(c_i)\n",
        "     \\]\n",
        "     Where \\( p(c_i) \\) is the probability of class \\( c_i \\) in dataset \\( D \\).\n",
        "\n",
        "   - **Variance Reduction** (for regression):\n",
        "     \n",
        "     \\[\n",
        "     \\text{Variance}(D) = \\frac{1}{|D|} \\sum_{i=1}^{|D|} (y_i - \\bar{y})^2\n",
        "     \\]\n",
        "     Where \\( y_i \\) is the target value for each data point, and \\( \\bar{y} \\) is the mean of the target values in dataset \\( D \\).\n",
        "\n",
        "#### 3. **Choosing the Best Split**:\n",
        "   The goal is to select the feature and threshold that minimizes the impurity (or maximizes the information gain for entropy). To measure the quality of a split, we calculate the **impurity** for each subset formed by the split and compute the **weighted average** of those impurities. For classification, this is often done using the following formula:\n",
        "\n",
        "   \\[\n",
        "   \\text{Weighted Impurity of Split} = \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} \\times \\text{Impurity}(D_v)\n",
        "   \\]\n",
        "   Where:\n",
        "   - \\( A \\) is the feature being considered for the split.\n",
        "   - \\( D_v \\) is the subset of data where the feature \\( A \\) takes the value \\( v \\).\n",
        "   - \\( |D_v| \\) and \\( |D| \\) are the number of instances in subset \\( D_v \\) and the total dataset \\( D \\), respectively.\n",
        "\n",
        "   We then choose the feature that minimizes this impurity and split the data accordingly.\n",
        "\n",
        "#### 4. **Recursion (Building the Tree)**:\n",
        "   After splitting the data based on the selected feature, the process is applied recursively to each resulting subset:\n",
        "   - For each subset, the algorithm checks if further splitting is necessary. If a subset is pure (i.e., all instances belong to the same class), the recursion stops, and a leaf node is created.\n",
        "   - The process continues until one of the stopping criteria is met (e.g., a maximum tree depth, minimum number of samples per leaf, or no further impurity reduction can be achieved).\n",
        "\n",
        "#### 5. **Stopping Criteria**:\n",
        "   Several stopping criteria can be used in decision trees, such as:\n",
        "   - Maximum tree depth (to limit how deep the tree can grow).\n",
        "   - Minimum samples per leaf (to prevent overfitting).\n",
        "   - Minimum impurity decrease (to stop when further splits do not reduce impurity enough).\n",
        "   - Maximum number of nodes or leaves.\n",
        "\n",
        "### Decision Tree Algorithm Overview\n",
        "\n",
        "Here’s a step-by-step overview of how a decision tree is built:\n",
        "\n",
        "1. **Start at the Root Node**:\n",
        "   - The entire dataset is at the root node.\n",
        "   \n",
        "2. **Select the Best Feature**:\n",
        "   - Evaluate all features and choose the one that gives the best split (i.e., the one that minimizes impurity or maximizes information gain).\n",
        "   \n",
        "3. **Split the Dataset**:\n",
        "   - Partition the dataset based on the chosen feature and threshold.\n",
        "   \n",
        "4. **Repeat the Process**:\n",
        "   - For each subset, repeat the process (select the best feature, split, and so on) until stopping criteria are met.\n",
        "   \n",
        "5. **Create Leaf Nodes**:\n",
        "   - Once the stopping criteria are met or the subsets are pure, assign a class label (for classification) or a predicted value (for regression) to the leaf node.\n",
        "\n",
        "### Example of a Simple Decision Tree for Classification\n",
        "\n",
        "Consider a simple dataset with two features \\( X_1 \\) and \\( X_2 \\), and a binary target variable \\( Y \\).\n",
        "\n",
        "| \\( X_1 \\) | \\( X_2 \\) | \\( Y \\) |\n",
        "|:--------:|:--------:|:------:|\n",
        "| 1        | 2        | 0      |\n",
        "| 2        | 3        | 0      |\n",
        "| 3        | 3        | 1      |\n",
        "| 4        | 4        | 1      |\n",
        "\n",
        "1. **Calculate Gini Impurity for the entire dataset**. Let’s say the Gini impurity of the root node is 0.5 (balanced classes).\n",
        "   \n",
        "2. **Split on \\( X_1 \\) = 2.5**:\n",
        "   - Left split: \\( X_1 \\leq 2.5 \\) → \\( \\{ (1, 2), (2, 3) \\} \\) → Gini = 0.0 (pure, all \\( Y = 0 \\)).\n",
        "   - Right split: \\( X_1 > 2.5 \\) → \\( \\{ (3, 3), (4, 4) \\} \\) → Gini = 0.0 (pure, all \\( Y = 1 \\)).\n",
        "\n",
        "3. **Choose the split with the lowest Gini impurity**:\n",
        "   - In this case, both splits are pure, so we stop and assign the class labels to the leaves.\n",
        "\n",
        "4. **The decision tree** looks like:\n",
        "\n",
        "   ```\n",
        "        X1 <= 2.5\n",
        "       /        \\\n",
        "     Y=0       Y=1\n",
        "   ```\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "A decision tree is a powerful algorithm that recursively splits data based on features to minimize impurity or uncertainty. The splitting criteria (like Gini Impurity or Entropy) are used to measure how well a feature divides the data. The process continues until the data in each subset is pure or a stopping condition is met. Decision trees are easy to interpret and can be very effective for both classification and regression tasks, but they can also suffer from overfitting, which is why techniques like pruning or ensemble methods (e.g., Random Forests) are often used to improve performance."
      ],
      "metadata": {
        "id": "CfHwlp6WqaSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.8 What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "5peFTznuqm9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.8 **Pre-pruning** (also known as **early stopping**) in decision trees is a technique used to **prevent overfitting** during the tree construction process by stopping the growth of the tree before it becomes too complex. The goal of pre-pruning is to ensure the decision tree doesn't grow excessively and capture noise in the training data, which can lead to poor generalization to unseen data.\n",
        "\n",
        "### How Pre-Pruning Works:\n",
        "\n",
        "In pre-pruning, the decision tree construction process is stopped at certain points before the tree reaches its full potential (i.e., before it perfectly classifies the training data). Several **stopping criteria** can be applied to decide when to stop growing the tree. Some common criteria include:\n",
        "\n",
        "1. **Maximum Depth**:\n",
        "   - The tree is stopped from growing further when a certain depth is reached. Depth is the longest path from the root to a leaf node. Limiting the depth can prevent the tree from becoming too deep and complex.\n",
        "   \n",
        "2. **Minimum Samples per Split**:\n",
        "   - A node is only split if it contains more than a specified minimum number of samples. If a node has fewer than the minimum samples required, it becomes a leaf node. This prevents the tree from creating nodes that represent very small, potentially noisy subsets of the data.\n",
        "\n",
        "3. **Minimum Samples per Leaf**:\n",
        "   - After a split, if the resulting subset contains fewer than the specified number of samples, the split is not performed, and that node becomes a leaf. This prevents the tree from growing based on very small subsets, which could be overfitting to noise.\n",
        "\n",
        "4. **Maximum Number of Leaf Nodes**:\n",
        "   - The growth of the tree is stopped once a maximum number of leaf nodes is reached. This limit can control the complexity of the tree and prevent overfitting.\n",
        "\n",
        "5. **Maximum Impurity Decrease**:\n",
        "   - A split is made only if the resulting split results in a decrease in impurity (like Gini Impurity or Entropy) by more than a specified threshold. If the improvement in impurity is too small, the split is not made.\n",
        "\n",
        "6. **Cost Complexity Pruning (Post-Pruning)**:\n",
        "   - This is another pruning strategy, but it occurs after the tree has been fully grown. In pre-pruning, the focus is on halting tree growth early, while post-pruning involves removing branches after the tree is fully grown, based on a cost-complexity measure.\n",
        "\n",
        "### Advantages of Pre-Pruning:\n",
        "- **Prevents Overfitting**: By stopping the tree from growing too large, pre-pruning helps to avoid overfitting the training data, making the model more generalizable.\n",
        "- **Faster Training**: Since the tree is stopped before it grows too large, training a decision tree with pre-pruning is usually faster.\n",
        "- **Simpler Model**: A pruned tree with fewer nodes is easier to interpret and visualize, which is one of the main advantages of decision trees.\n",
        "\n",
        "### Disadvantages of Pre-Pruning:\n",
        "- **Underfitting**: If the stopping criteria are too strict, the tree may be too simple and may fail to capture important patterns in the data, leading to underfitting.\n",
        "- **Suboptimal Tree**: Pre-pruning may stop the tree from growing before it reaches the best possible configuration. It may not fully exploit the data, and as a result, the performance could be suboptimal.\n",
        "\n",
        "### Example of Pre-Pruning in Action:\n",
        "\n",
        "Let's assume you are building a decision tree to classify a dataset of customer information (e.g., age, income, spending habits) into two classes: \"high spender\" or \"low spender.\"\n",
        "\n",
        "- **Without Pre-Pruning**: The decision tree may grow deeply and perfectly classify the training data, but it could become too complex, capturing patterns that are just noise or artifacts in the training data (overfitting). It might have deep branches that don't generalize well to new data.\n",
        "  \n",
        "- **With Pre-Pruning**: You set a limit on the tree's depth (e.g., a maximum depth of 3). The tree will stop growing once it reaches this depth, and some data points may not be perfectly classified, but the model will likely generalize better to new, unseen data.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- **Pre-pruning** involves halting the tree's growth early based on certain conditions (e.g., maximum depth, minimum samples per split, etc.).\n",
        "- It helps prevent overfitting by keeping the model simpler and more generalizable.\n",
        "- However, it can lead to underfitting if the pruning criteria are too strict, which may result in a tree that is too simple to capture the underlying patterns in the data.\n",
        "\n",
        "By carefully choosing the right pre-pruning parameters, you can balance model complexity and accuracy, leading to better performance on unseen data."
      ],
      "metadata": {
        "id": "qKfd7FGfrFOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.9 What is Post-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "c9N3m-MCrMz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.9 **Post-pruning** (also known as **cost-complexity pruning** or **post-pruning** in decision trees) is a technique used to **reduce the complexity of a fully grown decision tree** by removing some of its branches after the tree has been built. Unlike **pre-pruning**, where the tree's growth is stopped early during the construction phase, post-pruning involves allowing the tree to grow fully and then pruning (removing) parts of the tree that might not improve the model's performance or may be overfitting to noise.\n",
        "\n",
        "### How Post-Pruning Works:\n",
        "\n",
        "1. **Grow the Full Tree**:\n",
        "   First, the decision tree is grown fully without any restrictions (i.e., without pre-pruning). This results in a large tree, potentially overfitting the training data.\n",
        "\n",
        "2. **Prune the Tree**:\n",
        "   After the full tree has been built, branches (or subtrees) are removed in a way that **minimizes the overall error**. Pruning typically involves removing nodes that do not contribute much to the predictive accuracy and replacing them with leaf nodes. The process continues until the tree is optimized for generalization.\n",
        "\n",
        "3. **Use a Cost-Complexity Measure**:\n",
        "   Post-pruning typically uses a **cost-complexity pruning** algorithm, which involves calculating a trade-off between the **size** of the tree and its **accuracy**. The aim is to find the smallest tree that performs well on both the training data and unseen data.\n",
        "\n",
        "   The cost-complexity criterion can be formalized as:\n",
        "\n",
        "   \\[\n",
        "   R_\\alpha(T) = R(T) + \\alpha \\times |\\text{leaves}(T)|\n",
        "   \\]\n",
        "\n",
        "   Where:\n",
        "   - \\( R(T) \\) is the error of the tree (usually misclassification rate or mean squared error for regression).\n",
        "   - \\( |\\text{leaves}(T)| \\) is the number of leaf nodes in the tree \\( T \\).\n",
        "   - \\( \\alpha \\) is a regularization parameter that controls the trade-off between tree size and error.\n",
        "   \n",
        "   The goal is to find the value of \\( \\alpha \\) that minimizes the combined cost, i.e., balancing between the tree’s accuracy and its complexity.\n",
        "\n",
        "4. **Iterative Pruning**:\n",
        "   - **Start at the leaf nodes**: Initially, prune the leaves and replace them with their parent nodes.\n",
        "   - **Evaluate performance**: After each pruning step, evaluate the performance of the tree (often using a validation set or cross-validation).\n",
        "   - **Stop when optimal**: Continue pruning and evaluating until further pruning reduces the model's performance or no further improvement can be made.\n",
        "\n",
        "### Steps in Post-Pruning (Cost-Complexity Pruning):\n",
        "\n",
        "1. **Build the Full Tree**:\n",
        "   - A decision tree is initially constructed without pre-pruning constraints, allowing it to grow to full size, even if it risks overfitting the training data.\n",
        "\n",
        "2. **Calculate the Cost-Complexity Measure**:\n",
        "   - For each internal node, calculate how much error would be reduced or increased by removing that node and replacing it with a leaf.\n",
        "   - This is done by comparing the performance of the tree before and after pruning.\n",
        "\n",
        "3. **Prune Subtrees**:\n",
        "   - After calculating the cost-complexity measure for each node, prune the subtree with the least beneficial contribution to the overall error. The subtree is replaced with a leaf node representing the majority class (or the mean value in case of regression).\n",
        "\n",
        "4. **Cross-Validation**:\n",
        "   - To determine the optimal pruning level, cross-validation is often used. The tree is pruned iteratively, and at each stage, its performance is evaluated on a validation set (data not used during training).\n",
        "   - The point at which the performance on the validation set is maximized (or error is minimized) is chosen as the final tree.\n",
        "\n",
        "5. **Final Tree**:\n",
        "   - Once the pruning process is complete, the resulting tree is a smaller, simpler version of the original tree, with fewer branches and more generalized predictions.\n",
        "\n",
        "### Advantages of Post-Pruning:\n",
        "- **Reduces Overfitting**: By pruning branches that may have been overfitted to noise or specific data points in the training set, post-pruning helps the model generalize better to unseen data.\n",
        "- **Improves Interpretability**: The resulting tree is smaller and simpler, making it easier to interpret and visualize.\n",
        "- **Flexibility**: Unlike pre-pruning, post-pruning allows the tree to explore all possible splits during the construction phase, which can sometimes lead to better results.\n",
        "\n",
        "### Disadvantages of Post-Pruning:\n",
        "- **Computationally Expensive**: Building the full tree before pruning can be computationally expensive, especially with large datasets. Additionally, the iterative process of pruning and evaluating performance requires more time and resources.\n",
        "- **Risk of Underfitting**: If the pruning is too aggressive (e.g., pruning too many branches), it may result in a tree that is too simple and does not capture the underlying patterns in the data, leading to underfitting.\n",
        "\n",
        "### Example of Post-Pruning in Action:\n",
        "\n",
        "Suppose we have a decision tree built to classify customer data into categories like \"Buy\" or \"Don’t Buy\". After building the full tree, you notice that certain branches are very specific to certain subsets of the data (e.g., \"if age > 40 and income > 80k, then predict 'Buy'\"). These branches may overfit to the training data and not generalize well.\n",
        "\n",
        "Using post-pruning, the tree is simplified by removing such branches. After pruning, the tree might say, \"if income > 50k, predict 'Buy'\", which generalizes better to new data, resulting in improved accuracy on test data.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- **Post-pruning** is a technique where a fully grown decision tree is pruned (simplified) after it has been built.\n",
        "- It aims to reduce **overfitting** by removing branches that don’t significantly contribute to the predictive accuracy.\n",
        "- The process uses **cost-complexity pruning** to balance tree size and error, typically incorporating cross-validation to determine the best level of pruning.\n",
        "- It leads to a simpler, more interpretable model, though it can be computationally intensive.\n",
        "\n",
        "By pruning unnecessary branches, post-pruning helps create a model that is more generalizable and less likely to overfit the training data, ultimately leading to better performance on unseen data."
      ],
      "metadata": {
        "id": "QUV1L47UrqyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.10 What is the difference between Pre-Pruning and Post-Pruning?"
      ],
      "metadata": {
        "id": "VQ98Sye6r0So"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.10 The primary difference between **pre-pruning** and **post-pruning** lies in **when** the pruning happens during the decision tree construction process. Both methods aim to prevent **overfitting** and **improve the generalization ability** of a decision tree, but they do so in different ways. Below are the key differences:\n",
        "\n",
        "### 1. **When Pruning Happens:**\n",
        "\n",
        "- **Pre-Pruning**:\n",
        "  - Pre-pruning (also known as **early stopping**) occurs **during** the tree-building process.\n",
        "  - The algorithm stops growing the tree as soon as certain conditions are met, preventing it from fully expanding.\n",
        "  - These conditions can include a maximum tree depth, a minimum number of samples required to split a node, or a minimum decrease in impurity required for a split.\n",
        "\n",
        "- **Post-Pruning**:\n",
        "  - Post-pruning (also known as **cost-complexity pruning**) occurs **after** the tree has been fully grown.\n",
        "  - The tree is built to its full depth, and then parts of it (often branches or subtrees) are pruned back to improve generalization.\n",
        "\n",
        "### 2. **Process of Tree Construction:**\n",
        "\n",
        "- **Pre-Pruning**:\n",
        "  - During the tree-building process, **splitting stops early** based on certain conditions, meaning the tree is less likely to overfit by becoming too complex.\n",
        "  - The decision to split a node is restricted based on predefined criteria (e.g., a minimum number of samples per node).\n",
        "  - This process may lead to underfitting if the conditions are too strict, because the tree might not capture all relevant patterns.\n",
        "\n",
        "- **Post-Pruning**:\n",
        "  - The tree is first **fully grown**, often with no restrictions on depth or node size. This allows the tree to explore all potential splits.\n",
        "  - After the tree has been built, **branches that do not significantly improve accuracy are removed**.\n",
        "  - Post-pruning usually leads to a tree with fewer nodes and branches, offering a better balance between model complexity and accuracy.\n",
        "\n",
        "### 3. **Tree Size and Complexity:**\n",
        "\n",
        "- **Pre-Pruning**:\n",
        "  - The tree tends to be **smaller and less complex** because it is stopped from growing early.\n",
        "  - However, this can lead to **underfitting** if the stopping conditions are too restrictive.\n",
        "  \n",
        "- **Post-Pruning**:\n",
        "  - The tree is **larger** initially since it grows without restriction.\n",
        "  - After pruning, the final tree is usually **simplified** by removing unnecessary branches, resulting in a more **balanced** tree that generalizes better than the fully grown version.\n",
        "\n",
        "### 4. **Risk of Overfitting and Underfitting:**\n",
        "\n",
        "- **Pre-Pruning**:\n",
        "  - **Reduces overfitting** by limiting the size of the tree, but **increases the risk of underfitting** if pruning criteria are too strict, meaning the tree may not capture enough complexity in the data.\n",
        "  \n",
        "- **Post-Pruning**:\n",
        "  - Allows the tree to grow fully, capturing all possible patterns in the training data. It then prunes unnecessary complexity, reducing the risk of overfitting while retaining useful patterns.\n",
        "  - Post-pruning generally offers better performance compared to pre-pruning in terms of balancing overfitting and underfitting.\n",
        "\n",
        "### 5. **Computational Efficiency:**\n",
        "\n",
        "- **Pre-Pruning**:\n",
        "  - Pre-pruning can be **faster** because the tree is stopped from growing early, leading to fewer calculations during the tree-building process.\n",
        "  \n",
        "- **Post-Pruning**:\n",
        "  - Post-pruning can be **more computationally expensive** because the tree must first be grown to its full depth, and then additional time is required to prune the tree.\n",
        "\n",
        "### 6. **Model Interpretation:**\n",
        "\n",
        "- **Pre-Pruning**:\n",
        "  - Pre-pruning generally results in a **simpler tree**, but if overly restrictive, it may omit useful patterns, leading to a model that’s easier to interpret but less accurate.\n",
        "  \n",
        "- **Post-Pruning**:\n",
        "  - Post-pruning typically results in a tree that is **more accurate** and better generalizes, though it can sometimes be more **difficult to interpret** if the pruning is aggressive.\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| **Feature**               | **Pre-Pruning**                              | **Post-Pruning**                             |\n",
        "|---------------------------|----------------------------------------------|---------------------------------------------|\n",
        "| **When pruning occurs**    | During tree construction (early stopping)    | After the tree has been fully grown         |\n",
        "| **Tree growth**            | Stops growing early based on criteria        | Fully grows the tree before pruning         |\n",
        "| **Tree complexity**        | Smaller, potentially underfitted            | Initially large, then pruned to simplify    |\n",
        "| **Risk of overfitting**    | Reduced overfitting (but may underfit)       | Reduced overfitting (after pruning)         |\n",
        "| **Risk of underfitting**   | Increased underfitting with strict criteria  | Less underfitting (pruning avoids overfitting) |\n",
        "| **Computational efficiency**| Faster (less tree construction)              | Slower (needs full tree construction first) |\n",
        "| **Accuracy**               | May be less accurate due to early stopping   | Typically more accurate after pruning       |\n",
        "| **Interpretability**       | Easier to interpret but may miss patterns    | More complex but can provide better generalization |\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- **Pre-pruning** is useful when you want to avoid overfitting and reduce computational cost by stopping the tree from growing too deep. However, it can result in underfitting if the constraints are too strict.\n",
        "- **Post-pruning** allows the tree to fully explore the data, then simplifies it to prevent overfitting, typically leading to a better-performing model.\n",
        "\n",
        "In practice, **post-pruning** is often preferred because it allows the tree to grow fully, then removes irrelevant parts, ensuring the tree captures all relevant patterns while avoiding excessive complexity."
      ],
      "metadata": {
        "id": "BVEOez6qr8rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.11 What is a Decision Tree Regressor?"
      ],
      "metadata": {
        "id": "FrOWzFX-sNfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.11 A **Decision Tree Regressor** is a type of decision tree model used for **regression tasks** in machine learning. Unlike a **Decision Tree Classifier**, which is used for classification tasks (predicting discrete labels or categories), a decision tree regressor is used to predict continuous numerical values.\n",
        "\n",
        "### How a Decision Tree Regressor Works:\n",
        "\n",
        "The Decision Tree Regressor follows a similar process to a decision tree classifier, but instead of predicting a class label, it predicts a real value (a continuous output). Here’s how it works:\n",
        "\n",
        "1. **Data Split**:\n",
        "   - The algorithm starts at the root node, where the entire dataset is used to make predictions.\n",
        "   - It then recursively splits the dataset into subsets based on the feature values that minimize a certain metric, like **mean squared error (MSE)** or **variance reduction**, at each node.\n",
        "\n",
        "2. **Splitting Criterion**:\n",
        "   - At each internal node, the decision tree algorithm selects the feature and threshold that results in the **best split**, meaning the resulting groups are as homogeneous as possible in terms of the target variable.\n",
        "   - The goal is to minimize the variance of the target variable within each subset.\n",
        "\n",
        "   In a regression setting, the commonly used splitting criterion is **Mean Squared Error (MSE)** or **Variance Reduction**, which can be calculated as:\n",
        "\n",
        "   \\[\n",
        "   \\text{Variance}(D) = \\frac{1}{|D|} \\sum_{i=1}^{|D|} (y_i - \\bar{y})^2\n",
        "   \\]\n",
        "   Where:\n",
        "   - \\( y_i \\) is the actual value for each data point,\n",
        "   - \\( \\bar{y} \\) is the mean of the target values in the dataset \\( D \\).\n",
        "\n",
        "3. **Prediction at Leaf Nodes**:\n",
        "   - Once the tree reaches a leaf node (a node that no longer splits), the model predicts the **mean value of the target variable** for all the samples in that node. This is the predicted value for any new input that reaches that leaf.\n",
        "   \n",
        "4. **Recursive Process**:\n",
        "   - The process of splitting continues recursively, with the algorithm choosing the feature and threshold that minimizes the variance (or maximizes the information gain) at each step, until one of the stopping conditions is met, such as reaching a maximum depth, minimum number of samples in a node, or minimum variance.\n",
        "\n",
        "### Key Parameters for Decision Tree Regressor:\n",
        "Several parameters are available to control the complexity of the tree and prevent overfitting:\n",
        "- **max_depth**: The maximum depth of the tree. Limiting depth can prevent overfitting.\n",
        "- **min_samples_split**: The minimum number of samples required to split an internal node.\n",
        "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node.\n",
        "- **max_features**: The number of features to consider when looking for the best split.\n",
        "- **criterion**: The function used to measure the quality of a split. For regression, this is typically \"mse\" (Mean Squared Error), \"friedman_mse\", or \"mae\" (Mean Absolute Error).\n",
        "- **max_leaf_nodes**: The maximum number of leaf nodes. Limiting this number can help control the size of the tree.\n",
        "- **splitter**: The strategy used to split at each node (e.g., \"best\" or \"random\").\n",
        "\n",
        "### Example:\n",
        "\n",
        "Let’s say you are trying to predict a house’s price based on features such as the number of rooms, location, square footage, etc. A decision tree regressor might follow these steps:\n",
        "\n",
        "1. The root node of the tree might split on a feature like \"square footage.\"\n",
        "2. If the square footage is above a certain threshold, it will branch to a node where it further splits on features like \"number of rooms.\"\n",
        "3. This recursive splitting continues until a stopping condition is met, such as a leaf node where a certain number of data points reside.\n",
        "4. The predicted value at each leaf node will be the **mean** of the house prices for all the data points in that node.\n",
        "\n",
        "### Advantages of Decision Tree Regressor:\n",
        "- **Non-linear Relationships**: Decision trees can capture non-linear relationships between features and target variables, unlike linear models.\n",
        "- **Interpretability**: Decision trees are easy to visualize and interpret because they break down the decision-making process into a series of simple decisions.\n",
        "- **No Feature Scaling**: Decision trees do not require feature scaling (e.g., normalization or standardization), which can be an advantage when working with raw features of varying scales.\n",
        "\n",
        "### Disadvantages of Decision Tree Regressor:\n",
        "- **Overfitting**: Decision trees are prone to overfitting, especially when the tree is very deep and complex. Overfitting happens when the model captures noise in the data and does not generalize well to new data.\n",
        "- **Instability**: Small changes in the data can result in large changes in the structure of the tree.\n",
        "- **Poor Performance on Extrapolation**: Decision trees can struggle when making predictions on unseen values that are outside the range of the training data.\n",
        "  \n",
        "### How to Prevent Overfitting:\n",
        "To reduce the risk of overfitting, techniques like **pre-pruning** (setting a maximum depth or a minimum number of samples per leaf) and **post-pruning** (removing branches that don’t significantly improve accuracy) can be used. **Ensemble methods** like **Random Forests** and **Gradient Boosting Machines (GBM)** are often preferred as they combine multiple trees and are less prone to overfitting.\n",
        "\n",
        "### Example Code (using `sklearn`):\n",
        "\n",
        "Here’s an example of how to use a **DecisionTreeRegressor** in Python with the `sklearn` library:\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate synthetic regression data\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and fit the model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "```\n",
        "\n",
        "### Summary:\n",
        "- A **Decision Tree Regressor** is used for predicting continuous numerical values.\n",
        "- It works by recursively splitting the data based on feature values to minimize variance within each subset.\n",
        "- The predicted value at each leaf node is the mean of the target variable for all data points in that node.\n",
        "- Decision tree regressors are interpretable and can capture non-linear relationships, but they can be prone to overfitting if not carefully controlled.\n"
      ],
      "metadata": {
        "id": "W81HFPonstNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.12 What are the advantages and disadvantages of Decision Trees?\n"
      ],
      "metadata": {
        "id": "DaTxNIujs3bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.12 ### **Advantages of Decision Trees**:\n",
        "\n",
        "1. **Easy to Understand and Interpret**:\n",
        "   - Decision trees are intuitive and easy to visualize. They make decisions based on a series of simple conditions (splits), which are easy for humans to understand, even without a background in data science.\n",
        "\n",
        "2. **No Feature Scaling Required**:\n",
        "   - Unlike many other machine learning algorithms (such as k-NN or SVM), decision trees do not require normalization or standardization of features. They can handle raw features without scaling them, making preprocessing easier.\n",
        "\n",
        "3. **Can Handle Both Categorical and Numerical Data**:\n",
        "   - Decision trees can handle both numerical and categorical data, which is an advantage when working with diverse datasets.\n",
        "\n",
        "4. **Non-Linear Relationships**:\n",
        "   - Decision trees can capture non-linear relationships between the features and the target variable. They do not assume any specific form of the relationship (e.g., linear or otherwise), unlike models like linear regression.\n",
        "\n",
        "5. **Handles Missing Values**:\n",
        "   - Some implementations of decision trees can handle missing data, either by making splits based on non-missing values or by using surrogate splits (alternative splits when a feature is missing).\n",
        "\n",
        "6. **Feature Importance**:\n",
        "   - Decision trees inherently perform feature selection and can provide insights into which features are the most important for predicting the target variable.\n",
        "\n",
        "7. **Robust to Outliers**:\n",
        "   - Decision trees are relatively robust to outliers, as they perform splits based on median or mean values, reducing the impact of extreme values.\n",
        "\n",
        "8. **Versatility**:\n",
        "   - They can be used for both **classification** (predicting categorical outcomes) and **regression** (predicting continuous outcomes), making them versatile.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Decision Trees**:\n",
        "\n",
        "1. **Overfitting**:\n",
        "   - One of the most significant disadvantages of decision trees is their tendency to overfit, especially when the tree is deep. If the tree is too complex, it may capture noise in the training data, leading to poor generalization to new data.\n",
        "   - Overfitting can be mitigated using pruning techniques (pre-pruning and post-pruning) or ensemble methods like Random Forests and Gradient Boosting.\n",
        "\n",
        "2. **Instability**:\n",
        "   - Decision trees are highly sensitive to small changes in the data. A small change in the dataset can lead to a completely different tree structure, making them unstable compared to some other models (like linear regression).\n",
        "\n",
        "3. **Bias Toward Features with More Levels**:\n",
        "   - Decision trees tend to favor features with many possible values (levels). For example, a feature with a large number of categories may dominate the decision tree splits, even if it's not the most predictive feature.\n",
        "\n",
        "4. **Poor Performance with Unstructured Data**:\n",
        "   - Decision trees can struggle when working with unstructured data like images, text, or audio. They are better suited for structured data (tabular format).\n",
        "\n",
        "5. **Limited to Rectangular Decision Boundaries**:\n",
        "   - In higher-dimensional spaces, decision trees create decision boundaries that are parallel to the feature axes. This results in decision boundaries that are often **rectangular** (axis-aligned) rather than curved, which might be suboptimal for some complex decision surfaces.\n",
        "\n",
        "6. **Greedy Nature**:\n",
        "   - Decision trees are **greedy algorithms**, meaning they make decisions based on local criteria (splitting data at each node), without considering the overall tree structure. This can sometimes lead to suboptimal trees.\n",
        "\n",
        "7. **Computationally Expensive**:\n",
        "   - For large datasets, especially those with many features, building a decision tree can become computationally expensive. The search for the best split at each node can take time, and the complexity increases as the tree grows deeper.\n",
        "\n",
        "8. **Difficulty in Handling Correlated Features**:\n",
        "   - If the features are highly correlated, decision trees might perform poorly, as they might end up splitting on one feature and ignoring the importance of others, or splitting on features that do not add much value.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**:\n",
        "\n",
        "| **Advantages**                          | **Disadvantages**                               |\n",
        "|-----------------------------------------|------------------------------------------------|\n",
        "| Easy to understand and interpret        | Prone to overfitting                           |\n",
        "| No feature scaling required             | Instability: small changes can change the tree |\n",
        "| Can handle both categorical and numerical data | Bias toward features with more levels        |\n",
        "| Can capture non-linear relationships    | Poor performance with unstructured data       |\n",
        "| Handles missing values (some implementations) | Limited to rectangular decision boundaries     |\n",
        "| Provides feature importance             | Greedy nature: local, not global optimization |\n",
        "| Robust to outliers                      | Computationally expensive for large datasets  |\n",
        "| Versatile for both classification and regression | Difficulty handling correlated features         |\n",
        "\n",
        "### Conclusion:\n",
        "Decision trees are a powerful and flexible model, but they need to be carefully tuned to avoid issues like overfitting and instability. For many real-world tasks, using decision trees as a base learner in **ensemble methods** (like Random Forests or Gradient Boosting) can help mitigate some of their disadvantages, while retaining their advantages."
      ],
      "metadata": {
        "id": "AL42F73ZvutW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.13 How does a Decision Tree handle missing values?"
      ],
      "metadata": {
        "id": "OOo64pxIv2lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.13 A **Decision Tree** can handle missing values in different ways during both the **training** and **prediction** stages. The method used depends on the specific implementation, but there are common strategies employed to deal with missing data.\n",
        "\n",
        "### Handling Missing Values During **Training**:\n",
        "\n",
        "1. **Surrogate Splits**:\n",
        "   - In many decision tree implementations, such as **CART (Classification and Regression Trees)**, surrogate splits are used. A **surrogate split** is a backup split that can be used when a value for the primary splitting feature is missing.\n",
        "   - When a decision tree encounters missing data at a node (i.e., a missing value for a feature that would typically be used for a split), it tries to use the surrogate split. The surrogate is typically the feature that is most correlated with the primary split.\n",
        "   - For example, if a decision tree is splitting based on a feature like **\"income\"** but the value is missing for some observations, it will look for another feature (say, **\"age\"**) that behaves similarly to **income** and use that as a surrogate to decide which branch to go down.\n",
        "\n",
        "2. **Assigning Missing Data to a Default Branch**:\n",
        "   - Another approach is to simply **assign missing values to a default branch** based on certain criteria. For example:\n",
        "     - If the missing value is for a categorical feature, the tree might place missing values in the most frequent category or in the \"majority\" branch.\n",
        "     - For numerical features, the missing values might be assigned to the branch corresponding to the **mean** or **median** value.\n",
        "\n",
        "3. **Splitting on Missing Values**:\n",
        "   - Some decision tree algorithms explicitly allow a **split based on the presence or absence of data** (i.e., missing versus not missing). For example:\n",
        "     - When splitting on a feature, the tree can create one branch for the instances where that feature is **not missing**, and another branch for instances where it **is missing**.\n",
        "   - This can sometimes provide useful information if the absence of a feature correlates with the target variable.\n",
        "\n",
        "4. **Imputation (Preprocessing)**:\n",
        "   - While not part of the decision tree algorithm itself, missing values are often imputed **before training**. Common imputation techniques include:\n",
        "     - **Mean imputation** (for numerical data) or **mode imputation** (for categorical data).\n",
        "     - **KNN imputation**, which imputes missing values by considering the values of the nearest neighbors.\n",
        "     - **Model-based imputation**, where a model (like a regression model) is used to predict the missing values based on other features.\n",
        "   - After imputing missing values, the decision tree is trained on the complete dataset as if there were no missing values.\n",
        "\n",
        "---\n",
        "\n",
        "### Handling Missing Values During **Prediction**:\n",
        "\n",
        "When predicting with a trained decision tree that encountered missing values during training, the process is slightly different:\n",
        "\n",
        "1. **Use Surrogate Splits (if they were used during training)**:\n",
        "   - If surrogate splits were employed during training, the tree will use the surrogate split at prediction time for any instance with missing values. This ensures that the tree still functions even when encountering missing data during prediction.\n",
        "   \n",
        "2. **Follow Default Branches**:\n",
        "   - If missing values were assigned to a default branch during training, the same logic applies during prediction. The tree will direct instances with missing values to the same default branch it used during training.\n",
        "\n",
        "3. **Branching on Missing Values**:\n",
        "   - If the model was trained with the ability to split based on whether a value is missing, the same branching logic applies to instances with missing values during prediction. The tree will take the \"missing\" branch for any missing values and the regular branch for non-missing values.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Methods for Handling Missing Values in Decision Trees:\n",
        "\n",
        "| **Method**                | **Description** |\n",
        "|---------------------------|-----------------|\n",
        "| **Surrogate Splits**       | Use another feature that is most correlated with the splitting feature as a backup when the primary feature is missing. |\n",
        "| **Default Branches**       | Assign missing values to a default branch based on some criteria, such as the majority class or the mean/median value. |\n",
        "| **Splitting on Missing**   | Split the data into two branches: one for missing values and one for non-missing values of a feature. |\n",
        "| **Imputation**             | Impute missing values (mean, median, or model-based imputation) before training the tree. |\n",
        "| **Use of Missing Indicator** | Create an additional binary feature indicating whether a value is missing for a given feature, and split based on this indicator. |\n",
        "\n",
        "---\n",
        "\n",
        "### Advantages and Disadvantages of These Approaches:\n",
        "\n",
        "- **Advantages**:\n",
        "  - These methods help ensure that the decision tree can continue making predictions even when data is missing, reducing the impact of missing values on model performance.\n",
        "  - Surrogate splits and handling missing data in the tree allow decision trees to adapt to the presence of missing data without requiring imputation or dropping rows.\n",
        "\n",
        "- **Disadvantages**:\n",
        "  - Some methods, like surrogate splits, can increase the complexity of the tree and may not always provide the best performance if the surrogate features are not good substitutes.\n",
        "  - Handling missing data within the tree can sometimes lead to suboptimal splits, as the model might not have the full information to make the best decision.\n",
        "  - Pre-imputation of missing data might introduce bias or noise if the imputation method is not chosen carefully, affecting the performance of the model.\n",
        "\n",
        "### Conclusion:\n",
        "Decision trees have several techniques for handling missing data, ranging from surrogate splits and default branches to imputation. The specific method chosen will depend on the tree implementation and the nature of the data. Proper handling of missing values can significantly improve the decision tree's ability to make accurate predictions."
      ],
      "metadata": {
        "id": "N_6m_6p8wArf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.14 How does a Decision Tree handle categorical features?"
      ],
      "metadata": {
        "id": "o_kR9eadwWGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.14 A **Decision Tree** handles categorical features by considering all possible categories or values of the categorical feature when splitting the data. Decision trees can handle both **nominal** (unordered) and **ordinal** (ordered) categorical features, but they do so in different ways depending on the type of categorical data.\n",
        "\n",
        "### **Handling Nominal (Unordered) Categorical Features:**\n",
        "\n",
        "For **nominal** categorical features, where the categories have no intrinsic order (e.g., \"color\" with categories \"red\", \"blue\", \"green\"), decision trees use the following methods:\n",
        "\n",
        "1. **Binary Splits** (One-vs-Rest or Multi-way Split):\n",
        "   - **One-vs-Rest Split**: A common approach for nominal categorical features is to treat each category separately and create a binary split based on whether the feature is equal to a particular category.\n",
        "   - For example, if a feature has three categories (\"red\", \"blue\", \"green\"), the decision tree might create binary splits like:\n",
        "     - **Is the color \"red\"?** (Yes/No split)\n",
        "     - **Is the color \"blue\"?** (Yes/No split)\n",
        "     - This effectively creates a separate decision rule for each category.\n",
        "   - Alternatively, the tree might perform a **multi-way split**, where each category is treated as a different branch, creating as many branches as there are categories.\n",
        "     - For example, the split can be based on:\n",
        "       - **Color = red** → Left branch\n",
        "       - **Color = blue** → Right branch\n",
        "       - **Color = green** → Further splitting or leaf node.\n",
        "\n",
        "2. **Entropy or Gini Impurity Calculation**:\n",
        "   - When considering how to split the data on a categorical feature, decision trees calculate the **impurity measure** (such as **Gini Impurity** or **Entropy**) for each possible split.\n",
        "   - For categorical features, the impurity is computed by looking at the proportions of data points that fall into each category and their corresponding target class distribution.\n",
        "     - For example, if splitting on the feature \"color\", the tree will compute how well each category of color separates the target variable, and choose the one that minimizes impurity or maximizes information gain.\n",
        "\n",
        "3. **One-Hot Encoding (in some implementations)**:\n",
        "   - In certain implementations, **one-hot encoding** might be applied to categorical variables before the tree is trained, especially when the decision tree implementation requires numerical input.\n",
        "   - One-hot encoding transforms categorical variables into multiple binary features (columns). For example, \"color\" with categories \"red\", \"blue\", and \"green\" would be transformed into three binary columns (Red, Blue, Green), each indicating whether the observation belongs to that category.\n",
        "\n",
        "### **Handling Ordinal (Ordered) Categorical Features:**\n",
        "\n",
        "For **ordinal** categorical features, where the categories have a meaningful order (e.g., \"low\", \"medium\", \"high\"), decision trees can treat the feature in two main ways:\n",
        "\n",
        "1. **Using Ordinal Nature in Splits**:\n",
        "   - The tree can **preserve the order** of the categories and treat them as an ordered feature. This allows the decision tree to make splits based on the **order** of categories, which can lead to more meaningful splits.\n",
        "   - For example, if the feature \"education level\" has categories \"high school\", \"bachelor's\", and \"master's\", the decision tree might split as:\n",
        "     - **Education = high school** → Left branch\n",
        "     - **Education = bachelor's** → Right branch\n",
        "     - **Education = master's** → Further splitting or leaf node.\n",
        "   - Here, the order is respected, and the tree can use it to form better splits based on the progression of categories.\n",
        "\n",
        "2. **Treating Ordinal Features as Numeric**:\n",
        "   - In some cases, especially when the number of categories is small, ordinal categorical features can be treated as **numeric** by assigning a numeric value to each category (e.g., \"low\" = 1, \"medium\" = 2, \"high\" = 3).\n",
        "   - The decision tree then splits based on these numeric values as if the feature were continuous. For example, a split might occur based on whether the education level is \"higher\" than a certain threshold.\n",
        "\n",
        "### **Steps for Handling Categorical Features in Decision Trees**:\n",
        "\n",
        "1. **Choosing the Best Split**:\n",
        "   - The decision tree algorithm evaluates all possible splits based on the categorical feature.\n",
        "   - For nominal features, it computes the impurity (e.g., Gini or Entropy) for each possible category (one-vs-rest or multi-way split) and selects the one that maximizes information gain or minimizes impurity.\n",
        "   - For ordinal features, it may use the inherent order of categories to determine which splits best separate the target variable.\n",
        "\n",
        "2. **Splitting the Data**:\n",
        "   - Once the best split is identified, the data is divided into subsets based on the selected categorical values. This process repeats recursively until a stopping condition is met (e.g., maximum tree depth, minimum samples per leaf, etc.).\n",
        "\n",
        "### **Advantages of Using Categorical Features in Decision Trees**:\n",
        "\n",
        "- **Handles Complex Relationships**: Decision trees are great for capturing complex relationships between categorical features and the target variable. They can create complex decision boundaries and handle non-linear relationships.\n",
        "- **Interpretability**: Categorical splits in decision trees are easy to interpret because each node in the tree corresponds to a decision based on a feature category.\n",
        "- **No Need for Feature Scaling**: Decision trees do not require any preprocessing like scaling or normalizing categorical variables, unlike models such as k-NN or SVM.\n",
        "\n",
        "### **Disadvantages**:\n",
        "\n",
        "- **Overfitting**: Decision trees can easily overfit when dealing with categorical features, especially if the number of categories is large or if the data is noisy. To avoid this, pruning or limiting the depth of the tree is often necessary.\n",
        "- **Sparse Data**: When categorical features have many unique values (especially if some categories have very few samples), decision trees may end up overfitting to small subsets of the data, making the model less generalizable.\n",
        "\n",
        "### **Summary of How Decision Trees Handle Categorical Features**:\n",
        "\n",
        "1. **Nominal (unordered) categorical features**:\n",
        "   - Decision trees either use binary splits (one-vs-rest) or multi-way splits based on the categories.\n",
        "   - They calculate impurity measures (like Gini or Entropy) for each possible split and choose the best one.\n",
        "\n",
        "2. **Ordinal (ordered) categorical features**:\n",
        "   - Decision trees may use the natural order of categories to make splits, or they may treat the categories numerically.\n",
        "   - The order is either preserved in the split (e.g., \"low\" < \"medium\" < \"high\") or encoded numerically.\n",
        "\n",
        "3. **Preprocessing (Optional)**:\n",
        "   - Categorical features can be one-hot encoded, especially in implementations that require numerical inputs.\n",
        "   - Some implementations may also use decision trees that inherently support categorical features without requiring one-hot encoding.\n",
        "\n",
        "Overall, decision trees are very flexible when handling categorical data and can deal with both nominal and ordinal types effectively, though they may require tuning to avoid overfitting in cases with many categories."
      ],
      "metadata": {
        "id": "Z9YVkcTLwwJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.15 What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "R_U4xh22xB6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.15 Decision Trees are a popular and powerful machine learning algorithm with a wide range of real-world applications due to their simplicity, interpretability, and ability to handle both classification and regression tasks. Here are some common real-world applications of decision trees:\n",
        "\n",
        "### 1. **Medical Diagnosis**:\n",
        "   - **Classification**: Decision trees are used in the healthcare industry to assist with medical diagnoses. They can help predict the presence or absence of diseases based on symptoms, test results, and patient history. For example, they can be used to determine whether a patient has a particular disease (e.g., diabetes, heart disease, cancer) based on diagnostic features.\n",
        "   - **Example**: Predicting if a patient has diabetes based on factors like age, BMI, blood pressure, and glucose levels.\n",
        "\n",
        "### 2. **Credit Scoring and Loan Approval**:\n",
        "   - **Classification**: Financial institutions use decision trees to evaluate the creditworthiness of individuals and make decisions about loan approvals. The tree can classify whether a loan should be approved based on historical data such as income, credit history, employment status, and other financial variables.\n",
        "   - **Example**: Predicting whether an applicant is likely to default on a loan based on their financial behavior, credit score, and employment history.\n",
        "\n",
        "### 3. **Fraud Detection**:\n",
        "   - **Classification**: Decision trees are widely used in fraud detection in various sectors like banking, insurance, and e-commerce. They help identify fraudulent activities by analyzing patterns of transactions or behaviors that deviate from normal activity.\n",
        "   - **Example**: Identifying fraudulent credit card transactions by analyzing the time, location, and frequency of purchases.\n",
        "\n",
        "### 4. **Customer Segmentation**:\n",
        "   - **Classification**: Businesses use decision trees to segment customers based on purchasing behavior, demographics, or interactions with the company. This allows for targeted marketing strategies.\n",
        "   - **Example**: Classifying customers into high-value or low-value segments based on their spending habits, frequency of purchase, and other attributes, which helps in personalized marketing campaigns.\n",
        "\n",
        "### 5. **Risk Management**:\n",
        "   - **Classification/Regression**: In industries such as finance and insurance, decision trees are used to assess and manage risks. They can help predict potential financial losses or insurance claims based on various risk factors.\n",
        "   - **Example**: Estimating the risk of a person filing an insurance claim based on their age, driving history, location, and type of insurance coverage.\n",
        "\n",
        "### 6. **Marketing and Sales**:\n",
        "   - **Classification**: Decision trees are used in marketing to predict customer responses to various marketing campaigns, helping businesses optimize their marketing efforts. They can also predict the likelihood of a customer purchasing a product or service.\n",
        "   - **Example**: Predicting which customers are likely to respond to a new promotional offer based on their past buying behavior and demographics.\n",
        "\n",
        "### 7. **Churn Prediction**:\n",
        "   - **Classification**: Decision trees are commonly used by companies in telecommunications, utilities, and subscription-based services to predict customer churn (i.e., when customers are likely to leave the service).\n",
        "   - **Example**: Predicting whether a customer will cancel a subscription or service based on factors such as usage patterns, payment history, and customer support interactions.\n",
        "\n",
        "### 8. **Supply Chain Management**:\n",
        "   - **Regression**: In supply chain management, decision trees can be used for predicting demand, optimizing inventory levels, and making decisions about ordering and logistics.\n",
        "   - **Example**: Forecasting the demand for a product during a specific time period to optimize stock levels and minimize stockouts or overstocking.\n",
        "\n",
        "### 9. **Manufacturing and Quality Control**:\n",
        "   - **Classification**: In manufacturing, decision trees are used for quality control, such as identifying defective products or predicting which products are likely to fail based on production data.\n",
        "   - **Example**: Classifying whether a batch of products will meet quality standards based on parameters such as raw material quality, machine settings, and production conditions.\n",
        "\n",
        "### 10. **Agriculture and Crop Management**:\n",
        "   - **Classification/Regression**: Decision trees are used in agriculture to predict crop yields, detect plant diseases, and optimize irrigation and fertilizer use based on environmental factors.\n",
        "   - **Example**: Predicting crop yields based on soil quality, weather conditions, and historical yield data to assist farmers in planning for harvest.\n",
        "\n",
        "### 11. **Energy Consumption and Efficiency**:\n",
        "   - **Regression**: Decision trees are used to predict energy consumption patterns, helping energy companies forecast demand and optimize the allocation of resources.\n",
        "   - **Example**: Predicting electricity consumption based on factors like weather conditions, time of day, and customer usage patterns to optimize energy grid management.\n",
        "\n",
        "### 12. **Gaming and Sports Analytics**:\n",
        "   - **Classification/Regression**: In sports analytics, decision trees are used to predict the outcomes of games, player performance, and other related factors. In video games, they are often used to control non-playable characters (NPCs) or AI behavior.\n",
        "   - **Example**: Predicting the outcome of a football game based on player statistics, team form, and historical performance.\n",
        "\n",
        "### 13. **Sentiment Analysis**:\n",
        "   - **Classification**: Decision trees can be used in natural language processing (NLP) tasks like sentiment analysis, where they classify text data (e.g., customer reviews or social media posts) into categories such as positive, negative, or neutral.\n",
        "   - **Example**: Classifying customer reviews of a product as \"positive\" or \"negative\" based on words and phrases in the review.\n",
        "\n",
        "### 14. **Environmental Monitoring**:\n",
        "   - **Classification/Regression**: Decision trees are used in environmental monitoring for tasks such as predicting air quality, water pollution levels, or the likelihood of natural disasters.\n",
        "   - **Example**: Predicting the likelihood of a forest fire occurring based on factors like temperature, humidity, wind speed, and vegetation type.\n",
        "\n",
        "### 15. **Product Recommendations**:\n",
        "   - **Classification/Regression**: Decision trees are often used in recommendation systems to predict which products a customer is most likely to purchase based on their previous purchases and preferences.\n",
        "   - **Example**: Recommending movies to a user based on their watching history and ratings.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Decision Trees in Real-World Applications**:\n",
        "1. **Interpretability**: Decision trees are easy to interpret and visualize, making them useful for applications where transparency and understanding of decision-making are crucial (e.g., in healthcare, finance, and legal applications).\n",
        "2. **Non-linearity**: Decision trees can capture complex, non-linear relationships between features, making them suitable for a wide range of problems.\n",
        "3. **Handling Mixed Data**: They can handle both numerical and categorical data without needing extensive preprocessing like normalization or one-hot encoding (though this can depend on the implementation).\n",
        "4. **Feature Importance**: Decision trees can provide insights into which features are most important in making predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Challenges in Decision Tree Applications**:\n",
        "1. **Overfitting**: Decision trees can overfit the data if they are too deep or too complex. This can be mitigated using techniques like pruning or using ensemble methods (e.g., Random Forests or Gradient Boosting).\n",
        "2. **Instability**: Small changes in the data can lead to drastically different tree structures. This can be reduced by using ensemble methods.\n",
        "3. **Bias toward Features with More Categories**: Decision trees can be biased toward features with many distinct values. Careful preprocessing or feature engineering is sometimes required.\n",
        "\n",
        "### **Conclusion**:\n",
        "Decision trees are versatile and powerful tools in machine learning, with applications across a wide range of industries, including healthcare, finance, marketing, manufacturing, and more. Their ability to handle both classification and regression tasks, along with their interpretability, makes them a popular choice for real-world applications."
      ],
      "metadata": {
        "id": "MDys6VKHxOQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions>"
      ],
      "metadata": {
        "id": "fsEKZ6qAJXON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy."
      ],
      "metadata": {
        "id": "QmaROaUW_k8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "PZiZz1Ct_uTj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2dc5f9-2a44-4307-d914-bb51f0066fb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.17 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print thefeature importances.\n",
        "\n"
      ],
      "metadata": {
        "id": "PC-I1iVfALKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with Gini impurity as the criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\")\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "  print(f\"Feature {i}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX9Hf2cpAheZ",
        "outputId": "73a921e1-9409-4a67-bed4-8b4d9c015ba7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "Feature 0: 0.01911001911001911\n",
            "Feature 1: 0.01911001911001911\n",
            "Feature 2: 0.5381637432635216\n",
            "Feature 3: 0.42361621851644027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.18 Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print themodel accuracy."
      ],
      "metadata": {
        "id": "TMBbcMt9AmTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier with Entropy as the criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\")\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxVfyEKQBPz-",
        "outputId": "c54495e2-f940-4ff6-8940-aadae04abb9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.19 Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using MeanSquared Error (MSE)*."
      ],
      "metadata": {
        "id": "gWog2L1iBUGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnjbKuN8Bjwg",
        "outputId": "638cd577-3479-4947-9d7c-e3a113694329"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.492466834878125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.20 Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz."
      ],
      "metadata": {
        "id": "tce62zLPBrxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset (assuming it's already loaded as in previous examples)\n",
        "# ... (previous code to load and train the model) ...\n",
        "\n",
        "# Export the decision tree to a DOT file\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a graphviz graph from the DOT data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Display the graph (in Colab, this will render the graph in the output)\n",
        "graph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "PECVin7dB5vz",
        "outputId": "a72b5f0c-a55f-4192-8888-3d483c66b7e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"852pt\" height=\"671pt\"\n viewBox=\"0.00 0.00 852.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 848,-667 848,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M513,-663C513,-663 378,-663 378,-663 372,-663 366,-657 366,-651 366,-651 366,-592 366,-592 366,-586 372,-580 378,-580 378,-580 513,-580 513,-580 519,-580 525,-586 525,-592 525,-592 525,-651 525,-651 525,-657 519,-663 513,-663\"/>\n<text text-anchor=\"start\" x=\"374\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"401.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.58</text>\n<text text-anchor=\"start\" x=\"400.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"387.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n<text text-anchor=\"start\" x=\"393\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M415,-536.5C415,-536.5 322,-536.5 322,-536.5 316,-536.5 310,-530.5 310,-524.5 310,-524.5 310,-480.5 310,-480.5 310,-474.5 316,-468.5 322,-468.5 322,-468.5 415,-468.5 415,-468.5 421,-468.5 427,-474.5 427,-480.5 427,-480.5 427,-524.5 427,-524.5 427,-530.5 421,-536.5 415,-536.5\"/>\n<text text-anchor=\"start\" x=\"328.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"327.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"318\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n<text text-anchor=\"start\" x=\"325\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M418.79,-579.91C411.38,-568.65 403.33,-556.42 395.88,-545.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"398.75,-543.1 390.33,-536.67 392.9,-546.94 398.75,-543.1\"/>\n<text text-anchor=\"middle\" x=\"385.28\" y=\"-557.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M587.5,-544C587.5,-544 457.5,-544 457.5,-544 451.5,-544 445.5,-538 445.5,-532 445.5,-532 445.5,-473 445.5,-473 445.5,-467 451.5,-461 457.5,-461 457.5,-461 587.5,-461 587.5,-461 593.5,-461 599.5,-467 599.5,-473 599.5,-473 599.5,-532 599.5,-532 599.5,-538 593.5,-544 587.5,-544\"/>\n<text text-anchor=\"start\" x=\"453.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"482.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n<text text-anchor=\"start\" x=\"468\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n<text text-anchor=\"start\" x=\"470\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M472.21,-579.91C478.01,-571.1 484.2,-561.7 490.18,-552.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"493.26,-554.3 495.83,-544.02 487.41,-550.45 493.26,-554.3\"/>\n<text text-anchor=\"middle\" x=\"500.88\" y=\"-564.81\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#54e992\" stroke=\"black\" d=\"M478,-425C478,-425 343,-425 343,-425 337,-425 331,-419 331,-413 331,-413 331,-354 331,-354 331,-348 337,-342 343,-342 343,-342 478,-342 478,-342 484,-342 490,-348 490,-354 490,-354 490,-413 490,-413 490,-419 484,-425 478,-425\"/>\n<text text-anchor=\"start\" x=\"339\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"363\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.535</text>\n<text text-anchor=\"start\" x=\"369.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 36, 5]</text>\n<text text-anchor=\"start\" x=\"358\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M483.64,-460.91C474.87,-451.74 465.47,-441.93 456.44,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"458.73,-429.82 449.29,-425.02 453.68,-434.66 458.73,-429.82\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M702,-425C702,-425 567,-425 567,-425 561,-425 555,-419 555,-413 555,-413 555,-354 555,-354 555,-348 561,-342 567,-342 567,-342 702,-342 702,-342 708,-342 714,-348 714,-354 714,-354 714,-413 714,-413 714,-419 708,-425 702,-425\"/>\n<text text-anchor=\"start\" x=\"563\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"587\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.196</text>\n<text text-anchor=\"start\" x=\"593.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"584\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 32]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>2&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M561.36,-460.91C570.13,-451.74 579.53,-441.93 588.56,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"591.32,-434.66 595.71,-425.02 586.27,-429.82 591.32,-434.66\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#3fe685\" stroke=\"black\" d=\"M256.5,-306C256.5,-306 134.5,-306 134.5,-306 128.5,-306 122.5,-300 122.5,-294 122.5,-294 122.5,-235 122.5,-235 122.5,-229 128.5,-223 134.5,-223 134.5,-223 256.5,-223 256.5,-223 262.5,-223 268.5,-229 268.5,-235 268.5,-235 268.5,-294 268.5,-294 268.5,-300 262.5,-306 256.5,-306\"/>\n<text text-anchor=\"start\" x=\"130.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.6</text>\n<text text-anchor=\"start\" x=\"148\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.187</text>\n<text text-anchor=\"start\" x=\"154.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 35</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 34, 1]</text>\n<text text-anchor=\"start\" x=\"143\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M335.91,-341.91C317.06,-331.65 296.72,-320.58 277.53,-310.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"278.97,-306.94 268.51,-305.23 275.62,-313.09 278.97,-306.94\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M475.5,-306C475.5,-306 345.5,-306 345.5,-306 339.5,-306 333.5,-300 333.5,-294 333.5,-294 333.5,-235 333.5,-235 333.5,-229 339.5,-223 345.5,-223 345.5,-223 475.5,-223 475.5,-223 481.5,-223 487.5,-229 487.5,-235 487.5,-235 487.5,-294 487.5,-294 487.5,-300 481.5,-306 475.5,-306\"/>\n<text text-anchor=\"start\" x=\"341.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"363\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.918</text>\n<text text-anchor=\"start\" x=\"373\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"363.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"362\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M410.5,-341.91C410.5,-333.65 410.5,-324.86 410.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"414,-316.02 410.5,-306.02 407,-316.02 414,-316.02\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/>\n<text text-anchor=\"start\" x=\"20.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 34</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 34, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.66,-222.91C135.04,-211.1 120.17,-198.22 106.6,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108.62,-183.57 98.77,-179.67 104.03,-188.86 108.62,-183.57\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-179.5C240,-179.5 151,-179.5 151,-179.5 145,-179.5 139,-173.5 139,-167.5 139,-167.5 139,-123.5 139,-123.5 139,-117.5 145,-111.5 151,-111.5 151,-111.5 240,-111.5 240,-111.5 246,-111.5 252,-117.5 252,-123.5 252,-123.5 252,-167.5 252,-167.5 252,-173.5 246,-179.5 240,-179.5\"/>\n<text text-anchor=\"start\" x=\"155.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M195.5,-222.91C195.5,-212.2 195.5,-200.62 195.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"199,-189.67 195.5,-179.67 192,-189.67 199,-189.67\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M371,-179.5C371,-179.5 282,-179.5 282,-179.5 276,-179.5 270,-173.5 270,-167.5 270,-167.5 270,-123.5 270,-123.5 270,-117.5 276,-111.5 282,-111.5 282,-111.5 371,-111.5 371,-111.5 377,-111.5 383,-117.5 383,-123.5 383,-123.5 383,-167.5 383,-167.5 383,-173.5 377,-179.5 371,-179.5\"/>\n<text text-anchor=\"start\" x=\"286.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"289\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"279.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"278\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M381.36,-222.91C373.28,-211.65 364.49,-199.42 356.37,-188.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"358.99,-185.75 350.31,-179.67 353.3,-189.83 358.99,-185.75\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M548,-187C548,-187 413,-187 413,-187 407,-187 401,-181 401,-175 401,-175 401,-116 401,-116 401,-110 407,-104 413,-104 413,-104 548,-104 548,-104 554,-104 560,-110 560,-116 560,-116 560,-175 560,-175 560,-181 554,-187 548,-187\"/>\n<text text-anchor=\"start\" x=\"409\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"433\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.918</text>\n<text text-anchor=\"start\" x=\"443\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"433.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"428\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M434.79,-222.91C440.05,-214.1 445.68,-204.7 451.12,-195.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"454.13,-197.4 456.26,-187.02 448.12,-193.81 454.13,-197.4\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M461,-68C461,-68 364,-68 364,-68 358,-68 352,-62 352,-56 352,-56 352,-12 352,-12 352,-6 358,0 364,0 364,0 461,0 461,0 467,0 473,-6 473,-12 473,-12 473,-56 473,-56 473,-62 467,-68 461,-68\"/>\n<text text-anchor=\"start\" x=\"372.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"365.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M455.18,-103.73C449.74,-94.97 443.99,-85.7 438.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"441.43,-74.95 433.18,-68.3 435.48,-78.64 441.43,-74.95\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M592,-68C592,-68 503,-68 503,-68 497,-68 491,-62 491,-56 491,-56 491,-12 491,-12 491,-6 497,0 503,0 503,0 592,0 592,0 598,0 604,-6 604,-12 604,-12 604,-56 604,-56 604,-62 598,-68 592,-68\"/>\n<text text-anchor=\"start\" x=\"507.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"510\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"499\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M505.45,-103.73C510.81,-94.97 516.48,-85.7 521.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"524.89,-78.66 527.12,-68.3 518.92,-75 524.89,-78.66\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M697,-306C697,-306 572,-306 572,-306 566,-306 560,-300 560,-294 560,-294 560,-235 560,-235 560,-229 566,-223 572,-223 572,-223 697,-223 697,-223 703,-223 709,-229 709,-235 709,-235 709,-294 709,-294 709,-300 703,-306 697,-306\"/>\n<text text-anchor=\"start\" x=\"568\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"587\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.918</text>\n<text text-anchor=\"start\" x=\"597\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"587.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M634.5,-341.91C634.5,-333.65 634.5,-324.86 634.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"638,-316.02 634.5,-306.02 631,-316.02 638,-316.02\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M832,-298.5C832,-298.5 739,-298.5 739,-298.5 733,-298.5 727,-292.5 727,-286.5 727,-286.5 727,-242.5 727,-242.5 727,-236.5 733,-230.5 739,-230.5 739,-230.5 832,-230.5 832,-230.5 838,-230.5 844,-236.5 844,-242.5 844,-242.5 844,-286.5 844,-286.5 844,-292.5 838,-298.5 832,-298.5\"/>\n<text text-anchor=\"start\" x=\"745.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"744.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n<text text-anchor=\"start\" x=\"735\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 30]</text>\n<text text-anchor=\"start\" x=\"737\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>12&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M686.89,-341.91C702.27,-329.99 719.07,-316.98 734.37,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"736.93,-307.56 742.69,-298.67 732.65,-302.03 736.93,-307.56\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M679,-179.5C679,-179.5 590,-179.5 590,-179.5 584,-179.5 578,-173.5 578,-167.5 578,-167.5 578,-123.5 578,-123.5 578,-117.5 584,-111.5 590,-111.5 590,-111.5 679,-111.5 679,-111.5 685,-111.5 691,-117.5 691,-123.5 691,-123.5 691,-167.5 691,-167.5 691,-173.5 685,-179.5 679,-179.5\"/>\n<text text-anchor=\"start\" x=\"594.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"597\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"587.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M634.5,-222.91C634.5,-212.2 634.5,-200.62 634.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"638,-189.67 634.5,-179.67 631,-189.67 638,-189.67\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M818,-179.5C818,-179.5 721,-179.5 721,-179.5 715,-179.5 709,-173.5 709,-167.5 709,-167.5 709,-123.5 709,-123.5 709,-117.5 715,-111.5 721,-111.5 721,-111.5 818,-111.5 818,-111.5 824,-111.5 830,-117.5 830,-123.5 830,-123.5 830,-167.5 830,-167.5 830,-173.5 824,-179.5 818,-179.5\"/>\n<text text-anchor=\"start\" x=\"729.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\n<text text-anchor=\"start\" x=\"732\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"722.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"717\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 13&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>13&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M681.34,-222.91C694.96,-211.1 709.83,-198.22 723.4,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"725.97,-188.86 731.23,-179.67 721.38,-183.57 725.97,-188.86\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7a0b925b8e50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.21 Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n"
      ],
      "metadata": {
        "id": "WcyJqelYCAfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a fully grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier()\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Create a Decision Tree Classifier with a maximum depth of 3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "print(f\"Accuracy (fully grown tree): {accuracy_full}\")\n",
        "print(f\"Accuracy (max_depth=3): {accuracy_depth3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9BnnNsSCWZ5",
        "outputId": "5b9e938a-2804-4694-ea91-83bd8c09ca5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (fully grown tree): 1.0\n",
            "Accuracy (max_depth=3): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.22 Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree."
      ],
      "metadata": {
        "id": "sE_1cQSgCfLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier()\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Create a Decision Tree Classifier with min_samples_split=5\n",
        "clf_min_samples = DecisionTreeClassifier(min_samples_split=5)\n",
        "clf_min_samples.fit(X_train, y_train)\n",
        "y_pred_min_samples = clf_min_samples.predict(X_test)\n",
        "accuracy_min_samples = accuracy_score(y_test, y_pred_min_samples)\n",
        "\n",
        "print(f\"Accuracy (default tree): {accuracy_default}\")\n",
        "print(f\"Accuracy (min_samples_split=5): {accuracy_min_samples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfzA4CQICxm7",
        "outputId": "34e7e1ab-8214-466d-cd4f-865ac8571696"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (default tree): 1.0\n",
            "Accuracy (min_samples_split=5): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.23 Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data?\n"
      ],
      "metadata": {
        "id": "rjTYpgqEC3JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset (assuming it's already loaded)\n",
        "# ... (previous code to load the Iris dataset) ...\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# ... (previous code to split the data) ...\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and testing data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf_scaled = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model on the scaled data\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy of the scaled model\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Create a Decision Tree Classifier for unscaled data (as in previous examples)\n",
        "clf_unscaled = DecisionTreeClassifier()\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "print(f\"Accuracy (unscaled data): {accuracy_unscaled}\")\n",
        "print(f\"Accuracy (scaled data): {accuracy_scaled}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E-yoMkgDUJg",
        "outputId": "ccd319c8-4930-428c-cd25-0f0b7dfab761"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (unscaled data): 1.0\n",
            "Accuracy (scaled data): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.24 Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.\n"
      ],
      "metadata": {
        "id": "_4YP2CVlDZfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Load the Iris dataset (assuming it's already loaded)\n",
        "# ... (previous code to load the Iris dataset) ...\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# ... (previous code to split the data) ...\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Create a OneVsRestClassifier using the Decision Tree Classifier\n",
        "ovr_classifier = OneVsRestClassifier(dt_classifier)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Accuracy (One-vs-Rest): {accuracy_ovr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1YNK4EHDvjV",
        "outputId": "54da6e92-06b6-490f-908e-dae574b0beba"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (One-vs-Rest): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.25 Write a Python program to train a Decision Tree Classifier and display the feature importance scores."
      ],
      "metadata": {
        "id": "iIz8joEeD0Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Display feature importance scores\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfe3RBFtD_aE",
        "outputId": "c12064f8-4686-4006-fbdd-d730295408b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: 0.0\n",
            "Feature 1: 0.01911001911001911\n",
            "Feature 2: 0.8932635518001373\n",
            "Feature 3: 0.08762642908984374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.26 Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree."
      ],
      "metadata": {
        "id": "YfmoU1udEDDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Target variable\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor with max_depth=5\n",
        "regressor_depth5 = DecisionTreeRegressor(max_depth=5)\n",
        "regressor_depth5.fit(X_train, y_train)\n",
        "y_pred_depth5 = regressor_depth5.predict(X_test)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "\n",
        "# Create a Decision Tree Regressor with no restrictions (default)\n",
        "regressor_unrestricted = DecisionTreeRegressor()\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
        "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
        "\n",
        "print(f\"Mean Squared Error (max_depth=5): {mse_depth5}\")\n",
        "print(f\"Mean Squared Error (unrestricted): {mse_unrestricted}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTFsSW_pGVxr",
        "outputId": "4c47c478-1880-4d2f-b8db-9e357a085241"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (max_depth=5): 0.5245146178314735\n",
            "Mean Squared Error (unrestricted): 0.5032848338073401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.27 Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy."
      ],
      "metadata": {
        "id": "YWSrYph4GdH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a decision tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Train decision trees with different ccp_alphas\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Evaluate the accuracy of each tree\n",
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "# Plot the accuracy vs. ccp_alpha\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\", drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "NWzk-h4yGwbw",
        "outputId": "93f689a9-f32e-4ad6-c9b7-26d5274ad5f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.28 Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score."
      ],
      "metadata": {
        "id": "OGuF5MueG1Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate model performance using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')  # Weighted average for multi-class\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQbEtqwtHU_L",
        "outputId": "c08a6a63-0306-4e11-8511-c40304691a93"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn."
      ],
      "metadata": {
        "id": "TQY-WoQNH7R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming you have already trained your Decision Tree Classifier (clf) and have y_test and y_pred\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Create a heatmap using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "EdpdN4kmIUZm",
        "outputId": "bb9caebc-804e-41e9-b70b-9d0cb36b2166"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.30 Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "hZtrmlxrIcfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "# for max_depth and min_samples_split.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, None],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnBulXyzI-bX",
        "outputId": "ae49dde2-bb64-429b-e861-13c0f280b941"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Best score: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The End"
      ],
      "metadata": {
        "id": "7vGGLOaNJ1PL"
      }
    }
  ]
}